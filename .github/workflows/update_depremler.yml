name: Update Deprem Data

on:
  schedule:
    - cron: "*/30 * * * *" # Her 30 dakikada bir çalışır
  workflow_dispatch: # Manuel tetikleme için

jobs:
  update-depremler:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3
        # persist-credentials default true, push için gerekli

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.11

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install requests pandas beautifulsoup4 PyGithub

      - name: Fetch Kandilli data and update JSON
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
        run: |
          python - << 'EOF'
import requests
from bs4 import BeautifulSoup
import pandas as pd
import json
import os
from datetime import datetime
from github import Github

REPO_NAME = "burakbulut/deprem-veri-collector"  # Kendi kullanıcı adın ve repo adın
FILE_PATH = "public/data/deprem_data.json"
MAX_RECORDS = 1000

# Kandilli veri çek
url = "http://www.koeri.boun.edu.tr/scripts/lst0.asp"
response = requests.get(url)
response.encoding = "utf-8"
soup = BeautifulSoup(response.text, "html.parser")
lines = soup.find("pre").get_text().split("\n")

data = []
for line in lines[7:]:
    parts = line.split()
    if len(parts) >= 9:
        tarih = parts[0]
        saat = parts[1]
        enlem = float(parts[2])
        boylam = float(parts[3])
        derinlik = float(parts[4])
        ml = parts[6] if parts[6] != "-.-" else None
        yer = " ".join(parts[8:-1])
        cozum = parts[-1]
        data.append({
            "Tarih": tarih,
            "Saat": saat,
            "Enlem": enlem,
            "Boylam": boylam,
            "Derinlik": derinlik,
            "ML": ml,
            "Yer": yer,
            "Cozum": cozum
        })

df_new = pd.DataFrame(data)

# Eski JSON varsa oku
if os.path.exists(FILE_PATH):
    with open(FILE_PATH, "r", encoding="utf-8") as f:
        try:
            old_data = json.load(f)
            df_old = pd.DataFrame(old_data)
        except:
            df_old = pd.DataFrame()
else:
    df_old = pd.DataFrame()

# Birleştir ve duplicate temizle
df_combined = pd.concat([df_old, df_new], ignore_index=True)
df_combined.drop_duplicates(subset=["Tarih", "Saat", "Enlem", "Boylam"], inplace=True)

# Tarih ve saat datetime yap
df_combined['Datetime'] = pd.to_datetime(df_combined['Tarih'] + ' ' + df_combined['Saat'], errors='coerce')
df_combined.sort_values(by="Datetime", ascending=False, inplace=True)

# Limit
df_limited = df_combined.head(MAX_RECORDS)

# JSON kaydet
os.makedirs(os.path.dirname(FILE_PATH), exist_ok=True)
with open(FILE_PATH, "w", encoding="utf-8") as f:
    f.write(df_limited.to_json(orient="records", force_ascii=False, indent=2))

# GitHub push
g = Github(os.environ["GITHUB_TOKEN"])
repo = g.get_repo(REPO_NAME)
try:
    file = repo.get_contents(FILE_PATH)
    repo.update_file(FILE_PATH, f"Auto update {datetime.now()}", df_limited.to_json(orient="records", force_ascii=False, indent=2), file.sha, branch="main")
    print("✅ JSON dosyası güncellendi ve pushlandı.")
except:
    repo.create_file(FILE_PATH, f"First upload {datetime.now()}", df_limited.to_json(orient="records", force_ascii=False, indent=2), branch="main")
    print("✅ JSON dosyası oluşturuldu ve pushlandı.")
EOF

